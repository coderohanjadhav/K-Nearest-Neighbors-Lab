{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# K-Nearest Neighbors"}, {"metadata": {}, "cell_type": "markdown", "source": "Lets load required libraries"}, {"metadata": {}, "cell_type": "code", "source": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget -O teleCust1000t.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/teleCust1000t.csv", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "--2020-06-10 06:23:52--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/teleCust1000t.csv\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 37048 (36K) [text/csv]\nSaving to: \u2018teleCust1000t.csv\u2019\n\n100%[======================================>] 37,048      --.-K/s   in 0.07s   \n\n2020-06-10 06:23:53 (487 KB/s) - \u2018teleCust1000t.csv\u2019 saved [37048/37048]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df=pd.read_csv('teleCust1000t.csv')\ndf.head", "execution_count": 3, "outputs": [{"output_type": "execute_result", "execution_count": 3, "data": {"text/plain": "<bound method NDFrame.head of      region  tenure  age  marital  address  income  ed  employ  retire  \\\n0         2      13   44        1        9    64.0   4       5     0.0   \n1         3      11   33        1        7   136.0   5       5     0.0   \n2         3      68   52        1       24   116.0   1      29     0.0   \n3         2      33   33        0       12    33.0   2       0     0.0   \n4         2      23   30        1        9    30.0   1       2     0.0   \n5         2      41   39        0       17    78.0   2      16     0.0   \n6         3      45   22        1        2    19.0   2       4     0.0   \n7         2      38   35        0        5    76.0   2      10     0.0   \n8         3      45   59        1        7   166.0   4      31     0.0   \n9         1      68   41        1       21    72.0   1      22     0.0   \n10        2       5   33        0       10   125.0   4       5     0.0   \n11        3       7   35        0       14    80.0   2      15     0.0   \n12        1      41   38        1        8    37.0   2       9     0.0   \n13        2      57   54        1       30   115.0   4      23     0.0   \n14        2       9   46        0        3    25.0   1       8     0.0   \n15        1      29   38        1       12    75.0   5       1     0.0   \n16        3      60   57        0       38   162.0   2      30     0.0   \n17        3      34   48        0        3    49.0   2       6     0.0   \n18        2       1   24        0        3    20.0   1       3     0.0   \n19        1      26   29        1        3    77.0   4       2     0.0   \n20        3       6   30        0        7    16.0   3       1     0.0   \n21        1      68   52        1       17   120.0   1      24     0.0   \n22        3      53   33        0       10   101.0   5       4     0.0   \n23        3      55   48        1       19    67.0   1      25     0.0   \n24        3      14   43        1       18    36.0   1       5     0.0   \n25        2       1   21        0        0    33.0   2       0     0.0   \n26        2      42   40        0        7    37.0   2       8     0.0   \n27        3      25   33        1       11    31.0   1       5     0.0   \n28        1       9   21        1        1    17.0   2       2     0.0   \n29        2      13   33        1        9    19.0   4       0     0.0   \n..      ...     ...  ...      ...      ...     ...  ..     ...     ...   \n970       3      48   45        0       15    41.0   1      10     0.0   \n971       2       9   53        1       20    50.0   2      13     0.0   \n972       1      61   52        0       21    82.0   1      18     0.0   \n973       3      72   53        0       34   108.0   1      23     0.0   \n974       3      57   60        0       20    14.0   2      27     1.0   \n975       2      57   51        0       10    46.0   1      13     0.0   \n976       1      37   60        1        0    46.0   1      20     0.0   \n977       3      45   51        0       13   146.0   4      16     0.0   \n978       3      45   39        1       15    36.0   1      11     0.0   \n979       2      55   44        0       24    83.0   1      23     0.0   \n980       2      24   27        0        2    17.0   3       3     0.0   \n981       3       1   20        0        1    18.0   3       0     0.0   \n982       2      34   23        1        3    24.0   1       7     0.0   \n983       3      14   37        1        5    48.0   3       1     0.0   \n984       3       6   32        0       10    47.0   1      10     0.0   \n985       3      24   30        0        0    25.0   4       5     0.0   \n986       2       3   26        1        6    59.0   4       0     0.0   \n987       1       4   30        0        1    45.0   4       6     0.0   \n988       1      72   40        1       19   163.0   4      15     0.0   \n989       1      44   56        0        4    63.0   2       6     0.0   \n990       1      50   43        0        6    27.0   3       4     0.0   \n991       2      61   50        1       16   190.0   2      22     0.0   \n992       1      34   52        1        2   106.0   2      19     0.0   \n993       3      26   54        1       30    26.0   3       1     0.0   \n994       1      15   46        1       17    63.0   5       1     0.0   \n995       3      10   39        0        0    27.0   3       0     0.0   \n996       1       7   34        0        2    22.0   5       5     0.0   \n997       3      67   59        0       40   944.0   5      33     0.0   \n998       3      70   49        0       18    87.0   2      22     0.0   \n999       3      50   36        1        7    39.0   3       3     0.0   \n\n     gender  reside  custcat  \n0         0       2        1  \n1         0       6        4  \n2         1       2        3  \n3         1       1        1  \n4         0       4        3  \n5         1       1        3  \n6         1       5        2  \n7         0       3        4  \n8         0       5        3  \n9         0       3        2  \n10        1       1        1  \n11        1       1        3  \n12        1       3        1  \n13        1       3        4  \n14        1       2        1  \n15        0       4        2  \n16        0       1        3  \n17        1       3        3  \n18        0       1        1  \n19        0       4        4  \n20        1       1        2  \n21        0       2        1  \n22        1       2        4  \n23        0       3        1  \n24        0       5        3  \n25        1       3        3  \n26        1       1        4  \n27        0       4        3  \n28        1       3        1  \n29        1       2        2  \n..      ...     ...      ...  \n970       1       2        1  \n971       1       2        1  \n972       0       1        3  \n973       0       1        3  \n974       1       1        3  \n975       0       1        2  \n976       0       3        3  \n977       0       2        1  \n978       1       5        1  \n979       1       1        3  \n980       0       5        2  \n981       0       1        1  \n982       1       4        3  \n983       1       4        1  \n984       0       1        3  \n985       0       2        4  \n986       1       3        4  \n987       0       3        4  \n988       0       2        2  \n989       1       1        3  \n990       0       1        3  \n991       1       2        2  \n992       0       2        3  \n993       1       2        4  \n994       0       2        4  \n995       1       3        1  \n996       1       1        1  \n997       1       1        4  \n998       1       1        3  \n999       1       3        2  \n\n[1000 rows x 12 columns]>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Data Visualization and Analysis"}, {"metadata": {}, "cell_type": "markdown", "source": "Let\u2019s see how many of each class is in our data set"}, {"metadata": {}, "cell_type": "code", "source": "df['custcat'].value_counts()", "execution_count": 4, "outputs": [{"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "3    281\n1    266\n4    236\n2    217\nName: custcat, dtype: int64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### 281 Plus Service, 266 Basic-service, 236 Total Service, and 217 E-Service customers"}, {"metadata": {}, "cell_type": "markdown", "source": "You can easily explore your data using visualization techniques:"}, {"metadata": {}, "cell_type": "code", "source": "df.hist(column='income', bins=50)", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f1fe91b4160>]],\n      dtype=object)"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<Figure size 432x288 with 1 Axes>", "image/png": "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\n"}, "metadata": {"needs_background": "light"}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Feature set"}, {"metadata": {}, "cell_type": "markdown", "source": "Lets define feature sets, X:"}, {"metadata": {}, "cell_type": "code", "source": "df.columns", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "Index(['region', 'tenure', 'age', 'marital', 'address', 'income', 'ed',\n       'employ', 'retire', 'gender', 'reside', 'custcat'],\n      dtype='object')"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:"}, {"metadata": {}, "cell_type": "code", "source": "X = df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float)\nX[0:5]", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "array([[  2.,  13.,  44.,   1.,   9.,  64.,   4.,   5.,   0.,   0.,   2.],\n       [  3.,  11.,  33.,   1.,   7., 136.,   5.,   5.,   0.,   0.,   6.],\n       [  3.,  68.,  52.,   1.,  24., 116.,   1.,  29.,   0.,   1.,   2.],\n       [  2.,  33.,  33.,   0.,  12.,  33.,   2.,   0.,   0.,   1.,   1.],\n       [  2.,  23.,  30.,   1.,   9.,  30.,   1.,   2.,   0.,   0.,   4.]])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "What are our labels?"}, {"metadata": {}, "cell_type": "code", "source": "y = df['custcat'].values\ny[0:5]", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "array([1, 4, 3, 1, 3])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Normalize Data"}, {"metadata": {}, "cell_type": "markdown", "source": "Data Standardization give data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on distance of cases:"}, {"metadata": {}, "cell_type": "code", "source": "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\nX[0:5]", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "array([[-0.02696767, -1.055125  ,  0.18450456,  1.0100505 , -0.25303431,\n        -0.12650641,  1.0877526 , -0.5941226 , -0.22207644, -1.03459817,\n        -0.23065004],\n       [ 1.19883553, -1.14880563, -0.69181243,  1.0100505 , -0.4514148 ,\n         0.54644972,  1.9062271 , -0.5941226 , -0.22207644, -1.03459817,\n         2.55666158],\n       [ 1.19883553,  1.52109247,  0.82182601,  1.0100505 ,  1.23481934,\n         0.35951747, -1.36767088,  1.78752803, -0.22207644,  0.96655883,\n        -0.23065004],\n       [-0.02696767, -0.11831864, -0.69181243, -0.9900495 ,  0.04453642,\n        -0.41625141, -0.54919639, -1.09029981, -0.22207644,  0.96655883,\n        -0.92747794],\n       [-0.02696767, -0.58672182, -0.93080797,  1.0100505 , -0.25303431,\n        -0.44429125, -1.36767088, -0.89182893, -0.22207644, -1.03459817,\n         1.16300577]])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Train Test Split"}, {"metadata": {}, "cell_type": "markdown", "source": "Out of Sample Accuracy is the percentage of correct predictions that the model makes on data that that the model has NOT been trained on. Doing a train and test on the same dataset will most likely have low out-of-sample accuracy, due to the likelihood of being over-fit.\nIt is important that our models have a high, out-of-sample accuracy, because the purpose of any model, of course, is to make correct predictions on unknown data. So how can we improve out-of-sample accuracy? One way is to use an evaluation approach called Train/Test Split. Train/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. \nThis will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the data. It is more realistic for real world problems."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "Train set: (800, 11) (800,)\nTest set: (200, 11) (200,)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Classification"}, {"metadata": {}, "cell_type": "markdown", "source": "# K nearest neighbor (KNN)"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Import library"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.neighbors import KNeighborsClassifier", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Training"}, {"metadata": {}, "cell_type": "code", "source": "k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n           weights='uniform')"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Predicting "}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}